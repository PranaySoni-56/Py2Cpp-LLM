{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Generator\n",
        "\n",
        "The requirement: use a Frontier model to generate high performance C++ code from Python code"
      ],
      "metadata": {
        "id": "siaDv7hCxdZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kxIz42HqrknT"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama (this will download and execute the installation script)\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJDoMwlFyF6K",
        "outputId": "cc97fc1d-42ce-454c-a74d-5f46ae362b9f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tgz\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start thr Ollama service in background\n",
        "subprocess.Popen([\"ollama\",\"serve\"],stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Wait for the service to start\n",
        "time.sleep(5)\n",
        "print(\"Ollama service started!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEL4TqkAyNik",
        "outputId": "b588430f-9147-4d68-9891-8ab60b8fcbb3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama service started!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if ollama is running\n",
        "!ollama --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ6wyeNiyRkG",
        "outputId": "cea6a8df-0b3c-4d44-dc74-c5e2cc97daf7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ollama version is 0.13.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model\n",
        "!ollama pull qwen2.5-coder\n",
        "!ollama pull deepseek-coder-v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_3Izjq8yU68",
        "outputId": "ff94ba61-e020-4235-f76d-9547ef5c990d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjev6XeGyX2n",
        "outputId": "068f6698-d939-434b-95ef-635e60e5f313"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                        ID              SIZE      MODIFIED               \n",
            "deepseek-coder-v2:latest    63fb193b3a9b    8.9 GB    Less than a second ago    \n",
            "qwen2.5-coder:latest        dae161e27b0e    4.7 GB    Less than a second ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to client libraries\n",
        "\n",
        "MODEL = \"llama3.2:1b\"\n",
        "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
      ],
      "metadata": {
        "id": "E-Fk8mBnxt_I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"qwen2.5-coder\", \"deepseek-coder-v2\" ]\n",
        "\n",
        "clients = {\"qwen2.5-coder\": ollama, \"deepseek-coder-v2\": ollama}\n",
        "\n",
        "# Want to keep costs ultra-low? Replace this with models of your choice, using the examples from yesterday"
      ],
      "metadata": {
        "id": "rYfV8Djdynfq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System_info.py"
      ],
      "metadata": {
        "id": "LNjUhH5rzX02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# ------------------------- helpers -------------------------\n",
        "\n",
        "\n",
        "def _run(cmd, timeout=3):\n",
        "    \"\"\"Run a command safely. Returns stdout text or ''.\n",
        "    Accepts either a string (shell) or list (no shell).\"\"\"\n",
        "    try:\n",
        "        if isinstance(cmd, str):\n",
        "            return subprocess.check_output(\n",
        "                cmd, shell=True, text=True, stderr=subprocess.DEVNULL, timeout=timeout\n",
        "            ).strip()\n",
        "        else:\n",
        "            return subprocess.check_output(\n",
        "                cmd, shell=False, text=True, stderr=subprocess.DEVNULL, timeout=timeout\n",
        "            ).strip()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def _first_line(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    return s.splitlines()[0].strip() if s else \"\"\n",
        "\n",
        "\n",
        "def _which(name: str) -> str:\n",
        "    return shutil.which(name) or \"\"\n",
        "\n",
        "\n",
        "def _bool_from_output(s: str) -> bool:\n",
        "    return s.strip() in {\"1\", \"true\", \"True\", \"YES\", \"Yes\", \"yes\"}\n",
        "\n",
        "\n",
        "# ------------------------- OS & env -------------------------\n",
        "\n",
        "\n",
        "def _os_block():\n",
        "    sysname = platform.system()  # 'Windows', 'Darwin', 'Linux'\n",
        "    machine = platform.machine() or \"\"\n",
        "    release = platform.release() or \"\"\n",
        "    version = platform.version() or \"\"\n",
        "    kernel = release if sysname == \"Windows\" else (_run([\"uname\", \"-r\"]) or release)\n",
        "\n",
        "    distro = {\"name\": \"\", \"version\": \"\"}\n",
        "    if sysname == \"Linux\":\n",
        "        # Best-effort parse of /etc/os-release\n",
        "        try:\n",
        "            with open(\"/etc/os-release\", \"r\") as f:\n",
        "                data = {}\n",
        "                for line in f:\n",
        "                    if \"=\" in line:\n",
        "                        k, v = line.rstrip().split(\"=\", 1)\n",
        "                        data[k] = v.strip('\"')\n",
        "                distro[\"name\"] = data.get(\"PRETTY_NAME\") or data.get(\"NAME\", \"\")\n",
        "                distro[\"version\"] = data.get(\"VERSION_ID\") or data.get(\"VERSION\", \"\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # WSL / Rosetta detection (harmless if not present)\n",
        "    wsl = False\n",
        "    if sysname != \"Windows\":\n",
        "        try:\n",
        "            with open(\"/proc/version\", \"r\") as f:\n",
        "                v = f.read().lower()\n",
        "                wsl = (\"microsoft\" in v) or (\"wsl\" in v)\n",
        "        except Exception:\n",
        "            wsl = False\n",
        "\n",
        "    rosetta = False\n",
        "    if sysname == \"Darwin\":\n",
        "        rosetta = _bool_from_output(_run([\"sysctl\", \"-in\", \"sysctl.proc_translated\"]))\n",
        "\n",
        "    # Target triple (best effort)\n",
        "    target = \"\"\n",
        "    for cc in (\"clang\", \"gcc\"):\n",
        "        if _which(cc):\n",
        "            out = _run([cc, \"-dumpmachine\"])\n",
        "            if out:\n",
        "                target = _first_line(out)\n",
        "                break\n",
        "\n",
        "    return {\n",
        "        \"system\": sysname,\n",
        "        \"arch\": machine,\n",
        "        \"release\": release,\n",
        "        \"version\": version,\n",
        "        \"kernel\": kernel,\n",
        "        \"distro\": distro if sysname == \"Linux\" else None,\n",
        "        \"wsl\": wsl,\n",
        "        \"rosetta2_translated\": rosetta,\n",
        "        \"target_triple\": target,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------- package managers -------------------------\n",
        "\n",
        "\n",
        "def _package_managers():\n",
        "    sysname = platform.system()\n",
        "    pms = []\n",
        "    if sysname == \"Windows\":\n",
        "        for pm in (\"winget\", \"choco\", \"scoop\"):\n",
        "            if _which(pm):\n",
        "                pms.append(pm)\n",
        "    elif sysname == \"Darwin\":\n",
        "        if _run([\"xcode-select\", \"-p\"]):\n",
        "            pms.append(\"xcode-select (CLT)\")\n",
        "        for pm in (\"brew\", \"port\"):\n",
        "            if _which(pm):\n",
        "                pms.append(pm)\n",
        "    else:\n",
        "        for pm in (\"apt\", \"dnf\", \"yum\", \"pacman\", \"zypper\", \"apk\", \"emerge\"):\n",
        "            if _which(pm):\n",
        "                pms.append(pm)\n",
        "    return pms\n",
        "\n",
        "\n",
        "# ------------------------- CPU (minimal) -------------------------\n",
        "\n",
        "\n",
        "def _cpu_block():\n",
        "    sysname = platform.system()\n",
        "    brand = \"\"\n",
        "    # A simple brand/model read per OS; ignore failures\n",
        "    if sysname == \"Linux\":\n",
        "        brand = _run(\"grep -m1 'model name' /proc/cpuinfo | cut -d: -f2\").strip()\n",
        "    elif sysname == \"Darwin\":\n",
        "        brand = _run([\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"])\n",
        "    elif sysname == \"Windows\":\n",
        "        brand = _run('powershell -NoProfile -Command \"(Get-CimInstance Win32_Processor).Name\"')\n",
        "        if not brand:\n",
        "            brand = _run(\"wmic cpu get Name /value\").replace(\"Name=\", \"\").strip()\n",
        "\n",
        "    # Logical cores always available; physical is best-effort\n",
        "    cores_logical = os.cpu_count() or 0\n",
        "    cores_physical = 0\n",
        "    if sysname == \"Darwin\":\n",
        "        cores_physical = int(_run([\"sysctl\", \"-n\", \"hw.physicalcpu\"]) or \"0\")\n",
        "    elif sysname == \"Windows\":\n",
        "        cores_physical = int(\n",
        "            _run('powershell -NoProfile -Command \"(Get-CimInstance Win32_Processor).NumberOfCores\"')\n",
        "            or \"0\"\n",
        "        )\n",
        "    elif sysname == \"Linux\":\n",
        "        # This is a quick approximation; fine for our use (parallel -j suggestions)\n",
        "        try:\n",
        "            # Count unique \"core id\" per physical id\n",
        "            mapping = _run(\"LC_ALL=C lscpu -p=CORE,SOCKET | grep -v '^#'\").splitlines()\n",
        "            unique = set(tuple(line.split(\",\")) for line in mapping if \",\" in line)\n",
        "            cores_physical = len(unique) or 0\n",
        "        except Exception:\n",
        "            cores_physical = 0\n",
        "\n",
        "    # A tiny SIMD hint set (best-effort, optional)\n",
        "    simd = []\n",
        "    if sysname == \"Linux\":\n",
        "        flags = _run(\"grep -m1 'flags' /proc/cpuinfo | cut -d: -f2\")\n",
        "        if flags:\n",
        "            fset = set(flags.upper().split())\n",
        "            for x in (\"AVX512F\", \"AVX2\", \"AVX\", \"FMA\", \"SSE4_2\", \"NEON\", \"SVE\"):\n",
        "                if x in fset:\n",
        "                    simd.append(x)\n",
        "    elif sysname == \"Darwin\":\n",
        "        feats = (\n",
        "            (\n",
        "                _run([\"sysctl\", \"-n\", \"machdep.cpu.features\"])\n",
        "                + \" \"\n",
        "                + _run([\"sysctl\", \"-n\", \"machdep.cpu.leaf7_features\"])\n",
        "            )\n",
        "            .upper()\n",
        "            .split()\n",
        "        )\n",
        "        for x in (\"AVX512F\", \"AVX2\", \"AVX\", \"FMA\", \"SSE4_2\", \"NEON\", \"SVE\"):\n",
        "            if x in feats:\n",
        "                simd.append(x)\n",
        "    # On Windows, skip flags â€” brand typically suffices for MSVC /arch choice.\n",
        "\n",
        "    return {\n",
        "        \"brand\": brand.strip(),\n",
        "        \"cores_logical\": cores_logical,\n",
        "        \"cores_physical\": cores_physical,\n",
        "        \"simd\": sorted(set(simd)),\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------- toolchain presence -------------------------\n",
        "\n",
        "\n",
        "def _toolchain_block():\n",
        "    def ver_line(exe, args=(\"--version\",)):\n",
        "        p = _which(exe)\n",
        "        if not p:\n",
        "            return \"\"\n",
        "        out = _run([p, *args])\n",
        "        return _first_line(out)\n",
        "\n",
        "    gcc = ver_line(\"gcc\")\n",
        "    gpp = ver_line(\"g++\")\n",
        "    clang = ver_line(\"clang\")\n",
        "\n",
        "    # MSVC cl (only available inside proper dev shell; handle gracefully)\n",
        "    msvc_cl = \"\"\n",
        "    cl_path = _which(\"cl\")\n",
        "    if cl_path:\n",
        "        msvc_cl = _first_line(_run(\"cl 2>&1\"))\n",
        "\n",
        "    # Build tools (presence + short version line)\n",
        "    cmake = ver_line(\"cmake\")\n",
        "    ninja = _first_line(_run([_which(\"ninja\"), \"--version\"])) if _which(\"ninja\") else \"\"\n",
        "    make = ver_line(\"make\")\n",
        "\n",
        "    # Linker (we only care if lld is available)\n",
        "    lld = ver_line(\"ld.lld\")\n",
        "    return {\n",
        "        \"compilers\": {\"gcc\": gcc, \"g++\": gpp, \"clang\": clang, \"msvc_cl\": msvc_cl},\n",
        "        \"build_tools\": {\"cmake\": cmake, \"ninja\": ninja, \"make\": make},\n",
        "        \"linkers\": {\"ld_lld\": lld},\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------- public API -------------------------\n",
        "\n",
        "\n",
        "def retrieve_system_info():\n",
        "    \"\"\"\n",
        "    Returns a compact dict with enough info for an LLM to:\n",
        "      - Pick an install path (winget/choco/scoop, Homebrew/Xcode CLT, apt/dnf/...),\n",
        "      - Choose a compiler family (MSVC/clang/gcc),\n",
        "      - Suggest safe optimization flags (e.g., -O3/-march=native or MSVC /O2),\n",
        "      - Decide on a build system (cmake+ninja) and parallel -j value.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"os\": _os_block(),\n",
        "        \"package_managers\": _package_managers(),\n",
        "        \"cpu\": _cpu_block(),\n",
        "        \"toolchain\": _toolchain_block(),\n",
        "    }\n",
        "\n",
        "\n",
        "def rust_toolchain_info():\n",
        "    \"\"\"\n",
        "    Return a dict with Rust-related settings:\n",
        "      - presence and paths for rustc / cargo / rustup / rust-analyzer\n",
        "      - versions\n",
        "      - active/default toolchain (if rustup is present)\n",
        "      - installed targets\n",
        "      - common env vars (CARGO_HOME, RUSTUP_HOME, RUSTFLAGS, CARGO_BUILD_TARGET)\n",
        "      - simple execution examples\n",
        "    Works on Windows, macOS, and Linux. Uses the existing helpers: _run, _which, _first_line.\n",
        "    \"\"\"\n",
        "    info = {\n",
        "        \"installed\": False,\n",
        "        \"rustc\": {\"path\": \"\", \"version\": \"\", \"host_triple\": \"\", \"release\": \"\", \"commit_hash\": \"\"},\n",
        "        \"cargo\": {\"path\": \"\", \"version\": \"\"},\n",
        "        \"rustup\": {\n",
        "            \"path\": \"\",\n",
        "            \"version\": \"\",\n",
        "            \"active_toolchain\": \"\",\n",
        "            \"default_toolchain\": \"\",\n",
        "            \"toolchains\": [],\n",
        "            \"targets_installed\": [],\n",
        "        },\n",
        "        \"rust_analyzer\": {\"path\": \"\"},\n",
        "        \"env\": {\n",
        "            \"CARGO_HOME\": os.environ.get(\"CARGO_HOME\", \"\"),\n",
        "            \"RUSTUP_HOME\": os.environ.get(\"RUSTUP_HOME\", \"\"),\n",
        "            \"RUSTFLAGS\": os.environ.get(\"RUSTFLAGS\", \"\"),\n",
        "            \"CARGO_BUILD_TARGET\": os.environ.get(\"CARGO_BUILD_TARGET\", \"\"),\n",
        "        },\n",
        "        \"execution_examples\": [],\n",
        "    }\n",
        "\n",
        "    # Paths\n",
        "    rustc_path = _which(\"rustc\")\n",
        "    cargo_path = _which(\"cargo\")\n",
        "    rustup_path = _which(\"rustup\")\n",
        "    ra_path = _which(\"rust-analyzer\")\n",
        "\n",
        "    info[\"rustc\"][\"path\"] = rustc_path or \"\"\n",
        "    info[\"cargo\"][\"path\"] = cargo_path or \"\"\n",
        "    info[\"rustup\"][\"path\"] = rustup_path or \"\"\n",
        "    info[\"rust_analyzer\"][\"path\"] = ra_path or \"\"\n",
        "\n",
        "    # Versions & verbose details\n",
        "    if rustc_path:\n",
        "        ver_line = _first_line(_run([rustc_path, \"--version\"]))\n",
        "        info[\"rustc\"][\"version\"] = ver_line\n",
        "        verbose = _run([rustc_path, \"--version\", \"--verbose\"])\n",
        "        host = release = commit = \"\"\n",
        "        for line in verbose.splitlines():\n",
        "            if line.startswith(\"host:\"):\n",
        "                host = line.split(\":\", 1)[1].strip()\n",
        "            elif line.startswith(\"release:\"):\n",
        "                release = line.split(\":\", 1)[1].strip()\n",
        "            elif line.startswith(\"commit-hash:\"):\n",
        "                commit = line.split(\":\", 1)[1].strip()\n",
        "        info[\"rustc\"][\"host_triple\"] = host\n",
        "        info[\"rustc\"][\"release\"] = release\n",
        "        info[\"rustc\"][\"commit_hash\"] = commit\n",
        "\n",
        "    if cargo_path:\n",
        "        info[\"cargo\"][\"version\"] = _first_line(_run([cargo_path, \"--version\"]))\n",
        "\n",
        "    if rustup_path:\n",
        "        info[\"rustup\"][\"version\"] = _first_line(_run([rustup_path, \"--version\"]))\n",
        "        # Active toolchain\n",
        "        active = _first_line(_run([rustup_path, \"show\", \"active-toolchain\"]))\n",
        "        info[\"rustup\"][\"active_toolchain\"] = active\n",
        "\n",
        "        # Default toolchain (best effort)\n",
        "        # Try parsing `rustup toolchain list` and pick the line with \"(default)\"\n",
        "        tlist = _run([rustup_path, \"toolchain\", \"list\"]).splitlines()\n",
        "        info[\"rustup\"][\"toolchains\"] = [t.strip() for t in tlist if t.strip()]\n",
        "        default_tc = \"\"\n",
        "        for line in tlist:\n",
        "            if \"(default)\" in line:\n",
        "                default_tc = line.strip()\n",
        "                break\n",
        "        if not default_tc:\n",
        "            # Fallback: sometimes `rustup show` includes \"default toolchain: ...\"\n",
        "            for line in _run([rustup_path, \"show\"]).splitlines():\n",
        "                if \"default toolchain:\" in line:\n",
        "                    default_tc = line.split(\":\", 1)[1].strip()\n",
        "                    break\n",
        "        info[\"rustup\"][\"default_toolchain\"] = default_tc\n",
        "\n",
        "        # Installed targets\n",
        "        targets = _run([rustup_path, \"target\", \"list\", \"--installed\"]).split()\n",
        "        info[\"rustup\"][\"targets_installed\"] = targets\n",
        "\n",
        "    # Execution examples (only include what will work on this system)\n",
        "    exec_examples = []\n",
        "    if cargo_path:\n",
        "        exec_examples.append(f'\"{cargo_path}\" build')\n",
        "        exec_examples.append(f'\"{cargo_path}\" run')\n",
        "        exec_examples.append(f'\"{cargo_path}\" test')\n",
        "    if rustc_path:\n",
        "        exec_examples.append(f'\"{rustc_path}\" hello.rs -o hello')\n",
        "    info[\"execution_examples\"] = exec_examples\n",
        "\n",
        "    # Installed?\n",
        "    info[\"installed\"] = bool(rustc_path or cargo_path or rustup_path)\n",
        "\n",
        "    # Fill in default homes if env vars are empty but typical locations exist\n",
        "    def _maybe_default_home(env_val, default_basename):\n",
        "        if env_val:\n",
        "            return env_val\n",
        "        home = os.path.expanduser(\"~\") or \"\"\n",
        "        candidate = os.path.join(home, default_basename) if home else \"\"\n",
        "        return candidate if candidate and os.path.isdir(candidate) else \"\"\n",
        "\n",
        "    info[\"env\"][\"CARGO_HOME\"] = _maybe_default_home(info[\"env\"][\"CARGO_HOME\"], \".cargo\")\n",
        "    info[\"env\"][\"RUSTUP_HOME\"] = _maybe_default_home(info[\"env\"][\"RUSTUP_HOME\"], \".rustup\")\n",
        "\n",
        "    return info"
      ],
      "metadata": {
        "id": "Z9zI049WzExo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_info = retrieve_system_info()\n",
        "system_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DU8OC6zc3W",
        "outputId": "4c039f70-23c1-498d-e57e-76ce3957ffca"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'os': {'system': 'Linux',\n",
              "  'arch': 'x86_64',\n",
              "  'release': '6.6.105+',\n",
              "  'version': '#1 SMP Thu Oct  2 10:42:05 UTC 2025',\n",
              "  'kernel': '6.6.105+',\n",
              "  'distro': {'name': 'Ubuntu 22.04.4 LTS', 'version': '22.04'},\n",
              "  'wsl': False,\n",
              "  'rosetta2_translated': False,\n",
              "  'target_triple': 'x86_64-linux-gnu'},\n",
              " 'package_managers': ['apt'],\n",
              " 'cpu': {'brand': 'Intel(R) Xeon(R) CPU @ 2.00GHz',\n",
              "  'cores_logical': 2,\n",
              "  'cores_physical': 1,\n",
              "  'simd': ['AVX', 'AVX2', 'AVX512F', 'FMA', 'SSE4_2']},\n",
              " 'toolchain': {'compilers': {'gcc': 'gcc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0',\n",
              "   'g++': 'g++ (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0',\n",
              "   'clang': '',\n",
              "   'msvc_cl': ''},\n",
              "  'build_tools': {'cmake': 'cmake version 3.31.10',\n",
              "   'ninja': '',\n",
              "   'make': 'GNU Make 4.3'},\n",
              "  'linkers': {'ld_lld': ''}}}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compile_command = [\"g++\", \"-std=c++17\", \"-O3\", \"-march=native\", \"main.cpp\", \"-o\", \"main\"]\n",
        "run_command = [\"./main\"]"
      ],
      "metadata": {
        "id": "Ey-RNae4zlyr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And now, on with the main task"
      ],
      "metadata": {
        "id": "-osf9kjv0MUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "Your task is to convert Python code into high performance C++ code.\n",
        "Respond only with C++ code. Do not provide any explanation other than occasional comments.\n",
        "The C++ response needs to produce an identical output in the fastest possible time.\n",
        "\"\"\"\n",
        "\n",
        "def user_prompt_for(python):\n",
        "    return f\"\"\"\n",
        "Port this Python code to C++ with the fastest possible implementation that produces identical output in the least time.\n",
        "The system information is:\n",
        "{system_info}\n",
        "Your response will be written to a file called main.cpp and then compiled and executed; the compilation command is:\n",
        "{compile_command}\n",
        "Respond only with C++ code.\n",
        "Python code to port:\n",
        "\n",
        "```python\n",
        "{python}\n",
        "```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eVuT0sU90H3f"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_for(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
        "    ]"
      ],
      "metadata": {
        "id": "32MEd4w51svg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_output(cpp):\n",
        "    with open(\"main.cpp\", \"w\") as f:\n",
        "        f.write(cpp)"
      ],
      "metadata": {
        "id": "q1Ah95LG1wLy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def port(model, python):\n",
        "    client = clients[model]\n",
        "    reasoning_effort = \"high\" if 'gpt' in model else None\n",
        "    response = client.chat.completions.create(model=model, messages=messages_for(python), reasoning_effort=reasoning_effort)\n",
        "    reply = response.choices[0].message.content\n",
        "    reply = reply.replace('```cpp','').replace('```','')\n",
        "    write_output(reply)\n",
        "    return reply"
      ],
      "metadata": {
        "id": "Ljst1F_x12Ph"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi = \"\"\"\n",
        "import time\n",
        "\n",
        "def calculate(iterations, param1, param2):\n",
        "    result = 1.0\n",
        "    for i in range(1, iterations+1):\n",
        "        j = i * param1 - param2\n",
        "        result -= (1/j)\n",
        "        j = i * param1 + param2\n",
        "        result += (1/j)\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "result = calculate(200_000_000, 4, 1) * 4\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Result: {result:.12f}\")\n",
        "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hO1afPy717ub"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_python(code):\n",
        "    globals_dict = {\"__builtins__\": __builtins__}\n",
        "\n",
        "    buffer = io.StringIO()\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = buffer\n",
        "\n",
        "    try:\n",
        "        exec(code, globals_dict)\n",
        "        output = buffer.getvalue()\n",
        "    except Exception as e:\n",
        "        output = f\"Error: {e}\"\n",
        "    finally:\n",
        "        sys.stdout = old_stdout\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "NiRMkpPC2AR0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_and_run():\n",
        "    try:\n",
        "        subprocess.run(compile_command, check=True, text=True, capture_output=True)\n",
        "        print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
        "        print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
        "        print(subprocess.run(run_command, check=True, text=True, capture_output=True).stdout)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred:\\n{e.stderr}\")"
      ],
      "metadata": {
        "id": "e2WqVr2G2Hn9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as ui:\n",
        "    with gr.Row():\n",
        "        python = gr.Textbox(label=\"Python code:\", lines=28, value=pi)\n",
        "        cpp = gr.Textbox(label=\"C++ code:\", lines=28)\n",
        "    with gr.Row():\n",
        "        model = gr.Dropdown(models, label=\"Select model\", value=models[0])\n",
        "        convert = gr.Button(\"Convert code\")\n",
        "\n",
        "    convert.click(port, inputs=[model, python], outputs=[cpp])\n",
        "\n",
        "ui.launch(inbrowser=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "qqrxI2C32MFu",
        "outputId": "49ae6f82-a31c-48d8-b95a-ac73f47fb24d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b4c02ee42fdc1b07cd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b4c02ee42fdc1b07cd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compile_and_run()"
      ],
      "metadata": {
        "id": "2gREmRn-6EF8"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}